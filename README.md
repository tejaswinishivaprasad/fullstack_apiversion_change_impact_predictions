Impact AI Prototype

This repository contains the full prototype for my thesis project. It includes the AI analysis core written in Python, a Spring Boot backend that exposes the API endpoints, a React frontend for the dashboard, and a curated dataset that is used for local testing and reproducibility. This README explains what each part does and how to run everything on a local machine. The aim is to make it easy for examiners to understand and reproduce the workflow.

1. Prerequisites

Please install the following on your system:

Python 3.11 or newer

Node.js 18 or newer

Java 17 or newer

Maven or Gradle depending on the Spring project setup

Git

Optional but useful: jq for inspecting JSON files

All components run locally without internet once dependencies are installed.

2. Repository Structure

The repository follows a simple layout so it is easy to navigate.

impact-ai-repo/
  frontend/                React dashboard
  spring-backend/          Spring Boot API server
  ai-core/                 Python analysis code
  datasets/
    curated/               Small curated dataset used in thesis
    raw/                   Empty. Raw dataset is not checked in
  scripts/                 Helper scripts
  artifacts/               Output generated after analysis
  .github/workflows/       GitHub Actions for PR annotations
  README.md

3. Curated Dataset

The curated dataset is stored in datasets/curated. It contains a very small and controlled set of OpenAPI specification files and optional NDJSON files. These are used for demonstrating the analysis pipeline quickly. The raw dataset used during the full model training is very large so it is not committed to this repository. A fetch script can be added if needed.

To inspect curated files:

ls datasets/curated/canonical


These files are used by both the AI core CLI and the Spring backend. The curated data is suitable for demo, testing and reproducibility for evaluation.

4. Running the AI Core (Python)

The AI core performs the main tasks in the project. It compares two API specification files, generates atomic change events, extracts features, sends the features to the trained model, computes the predicted risk and writes a report file.

Steps to run:
cd ai-core
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt


Then generate a sample report:

python run_sample.py \
  --old ../datasets/curated/canonical/service-v1.json \
  --new ../datasets/curated/canonical/service-v2.json \
  --out ../artifacts/report.json


After it completes, check the output:

cat ../artifacts/report.json


This file is what the frontend and GitHub Actions workflow both use.

5. Running the Spring Backend

The Spring backend exposes a set of endpoints which the React dashboard calls. It also reads the curated dataset and serves files, metadata and analysis outputs.

Steps to run:

cd spring-backend
./mvnw clean package -DskipTests
java -jar target/*.jar


Confirm it is running by checking:

http://localhost:8080/health


If needed, adjust dataset root directory by setting an environment variable before starting:

export DATASETS_ROOT=/path/to/datasets

6. Running the React Frontend

The frontend is the dashboard used throughout the thesis. It visualizes the predicted risk, confidence, backend and frontend impact distributions, change details and model explanation.

Steps to start the frontend:

cd frontend
npm ci
npm start


It should open at:

http://localhost:3000


Make sure the backend is running on port 8080. If the backend is on a different port, update the value in the .env file:

REACT_APP_API_BASE_URL=http://localhost:8080


From the dashboard you can choose the old and new OpenAPI specification, run analysis and view the report. The dashboard can load the curated data instantly.

7. GitHub Actions and PR Annotation Demo

The repository includes a workflow that runs during pull requests. The workflow creates a sample analysis report and posts annotations directly on the pull request so reviewers can see predicted risks and change details without running anything locally.

The workflow file is at:

.github/workflows/pr-annotate.yml


To test it:

Push your code to GitHub

Create a new branch

Open a pull request against main

Wait for the workflow to run

Open the pull request checks page to see annotations

The workflow uses the Checks API and requires the correct permissions which are already included in the workflow file.

8. Expected Structure of report.json

A sample report.json generated by the AI core looks like this:

{
  "predicted_risk": 0.48,
  "confidence": {
    "overall": 0.72,
    "backend": 0.70,
    "frontend": 0.75
  },
  "details": [
    {
      "ace_id": "ace-001",
      "type": "ENDPOINT_ADDED",
      "detail": "/v1/new",
      "file": "spring-backend/src/main/java/.../ApiController.java",
      "start_line": 120,
      "end_line": 124,
      "risk_score": 0.75
    }
  ],
  "backend_impacts": [
    { "service": "svc:orders", "risk_score": 0.63 }
  ],
  "frontend_impacts": [
    { "service": "ui:admin", "risk_score": 0.21 }
  ],
  "metadata": {
    "dataset": "curated",
    "commit_hash": "abc123",
    "generated_at": "2025-11-16T18:00:00Z"
  }
}


The React dashboard expects this structure. The PR annotation workflow also reads the same structure.

9. Troubleshooting

If the frontend cannot fetch datasets, check if the Spring backend is running.
If the Spring backend cannot find files, confirm the curated dataset path.
If the PR annotations do not appear, check the workflow logs under GitHub Actions.
If pip fails to install dependencies, upgrade pip and retry.
If Node scripts fail, delete node_modules and run npm ci again.

10. Reproducibility Notes 

The curated dataset allows examiners to reproduce the demo without downloading a large raw dataset. 
The AI core can run entirely offline once dependencies are installed. 
The Spring and React components work locally and only depend on the AI core output. 
The PR annotation workflow can be tested by creating a throwaway branch and opening a pull request.
